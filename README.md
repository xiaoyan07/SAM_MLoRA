<h2 align="center">Multi-LoRA Fine-Tuned Segment Anything Model for Urban Man-Made Object Extraction</h2>

<h5 align="center"> <a href="https://scholar.google.com/citations?user=MDA37NMAAAAJ&hl=zh-CN">Xiaoyan LU</a>,
and <a href="https://scholar.google.com/citations?user=SbbCxE8AAAAJ">Qihao WENG</a></h5>


[[`Paper`]()] 

[The code and datasets will be released after the manuscript is accepted]

## Multi-LoRA Fine-Tuned SAM Framework

<div align="center">
  <img src="./img/SAM_LoRA.png?raw=true">
</div>

## The training dataset

1. [<b>DeepGlobe Road Dataset </b>](https://competitions.codalab.org/competitions/18467#participate-get_data): 4696 samples
2. [<b>SpaceNet Building Dataset </b>](https://spacenet.ai/spacenet-buildings-dataset-v2/): 8429 samples

If you have difficulty processing this data, feel free to reach out to me at xiaoyan07.lu@polyu.edu.hk for assistance.


## The validation set spans across five continents.

1. [<b>The WHU building (Christchurch) dataset</b>](http://gpcv.whu.edu.cn/data/building_dataset.html): 2416 samples
2. The other validation dataset: [<b>Baidu Drive</b>]( ), Code:

<div align="center">
  <img src="./img/val_data.png?raw=true">
</div>

## Pre-trained model
The pre-trained SAM_MLoRA is released at [<b>Baidu Drive</b>](), Code:
